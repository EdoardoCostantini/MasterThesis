%        File: model.tex

\documentclass[a4paper]{article}

% Packages
\usepackage{bm} % for math bold
\usepackage{amsmath} % for the allignment of equations under same number
\usepackage{amsmath}					 % matrix
\usepackage[a4paper, total={17cm, 23cm}]{geometry} %a4 = 21cm x 29.7cm


\begin{document}
Here I define the Bayesian model I want to use to test the performance of the different priors. The features I want to include are the folowing: continuous outcome, any number/measurement scale.

Let's start with the \textbf{model}

\begin{equation}
 \begin{split}
	y_{ij}& = \bm{x}^{T}_{ij} \bm{\theta} + \bm{z}^{T}_{ij}\bm{b}_i + \epsilon_{ij}\\
	\bm{b}_i& \sim N(\bm{0}, \bm{\Psi})\\
	\epsilon_{ij}& \sim N(0, \sigma^2)
 \end{split}
\end{equation}

(Vectors are in bold, matrix are capital Greek letters).\\
I'm going to define the following \textbf{priors}:

\begin{equation}
p(\bm{\theta}) \propto 1
\end{equation}
\begin{equation}
p(\sigma^2) \propto \sigma^{-2}
\end{equation}

For what concerns the random effects variance covariance matrix, different priors are tested. In particular we used:

\begin{itemize}
	\item inverse-Wishart
	
\begin{equation}
p(\bm{\Psi}) \propto IW(\nu_0, S_{0})
\end{equation}	

where we choose $\nu_0$ to be 1 and $S_{0}$ to be diag(2), following indications by Gelman \textit{et al} (2014). Thanks to the partitioning property, we know that the diagonal components of $\Psi$ have an inverse-Wisharet distribution themselves. Furthermore, for a univariate case, we know that an inverse Wishart distribution simplifies to an inverse Gamma with parameters $p=1, \alpha=\frac{\nu}{2}, \beta = S_{0kk}$. Finally, we can then apply Jacobian to find the corresponding prior distribution on the standard deviation of any random effect $\sigma_k$.

	\item inverse-Wishart \textit{a là} Huang and Wand

\begin{equation}
	\begin{split}
	p(\bm{\Psi|a_1, a_2})& \propto IW(\nu_0 + p - 1, 2\nu_0 diag(1/a_1, 1/a_2)), \\
	a_k& \propto IG(1/2,1/A_k^2),
	\end{split}
\end{equation}

with $\nu_0 = 2$ and $\bm{A} = [100, 100]$. Considering a $2 \times 2$ random effects variance covariance matrix (random intercepts, and random slopes) that has this prior distribution, the corresponding marginal distributions of the standard deviation random components have an half-Cauchy distribution (belonging to the half-\textit{t} family, see Gelman (2006) for details). In particular, any square-root diagonal element $\sigma_k$ of a matrix following such a distribution is Half-\textit{t}$(\nu_0, A_k)$. When, $\nu = 1$, and $A_k$ is equal to the prior guess we have an half-Cauchy prior distribution for a $\sigma_k$ parameter. Furthermore, the marginal priors for the correlations have a beta distribution with parameters $\alpha = -1, \beta = -1$ (Huang and Wand, 2013).
	
	\item Matrix-F variate
	
\begin{equation}
\begin{split}
p(\bm{\Psi})& \propto F(\bm{\Psi}; \nu_0, \delta, \bm{B}) \\
& \propto \int IW(\bm{\Psi}; \delta + k - 1, \Sigma) \times W(\bm{\Sigma}; \nu_0, \bm{B})d\bm{\Sigma}
\end{split}
\end{equation}	
\end{itemize}

where $\nu_0 = 2$, $\delta = 1$, and $\bm{B}$ is a prior guess. Three different choices where made for $\bm{B}$ in this paper: diag($10^3$), proper neighbor of $(\sigma^2)^{-\frac{1}{2}}$; $\bm{B}_{ed}$, an educated guess based on data exploration, $\bm{R^*}$ and an empirical bayes choice following Kass and Natarajan (2006). \\
Considering a $2 \times 2$ random effects variance covariance matrix (random intercepts, and random slopes) that is matrix-F distirbuted, $F(\nu_0, \delta, \bm{B})$, the marginal distribution on the standard deviations of the random effects are univariate $F(\nu_0, \delta, b_{11})$ and $F(\nu_0, \delta, b_{22})$, with $\nu_0 >1, \delta > 0, b_{jj} > 0$.

\vspace{5mm}

The derivation of the conditional posterior follows.

\paragraph{Full conditional for $\bm{\theta}$}(fixed effects) \\

Let's start with
 \begin{equation*}
	p(\bm{\theta}|\bm{y}, \bm{X}, \bm{Z}, \bm{b}_{i}, \bm{\Psi}, \sigma^2) = 	p(\bm{y}|\bm{\theta}, \bm{X}, \bm{Z}, \bm{b}_{i}, \bm{\Psi}, \sigma^2) p(\bm{\theta})
 \end{equation*}
where
 \begin{equation*}
  \begin{split}
  p(\bm{y}|\bm{\theta},\bm{X},\bm{Z},\bm{\Psi},\sigma^2) &= \prod_{i=1}^n \prod_{j=1}^Jp(y_{ij}|\bm{\theta}^{T}\bm{x}_{ij} + \bm{b}_{i}^{T}\bm{z}_{ij}, \bm{\Psi}, \sigma^2) \\
  &\propto exp(-\frac{1}{2\sigma^2}SSR)
  \end{split}
 \end{equation*}
and
 \begin{equation*}
  SSR = \sum_{i = 1}^{n}\sum_{j = 1}^{J}( y_{ij}-\bm{\theta}^{T}\bm{x}_{ij} - \bm{b}_{i}^{T}\bm{z}_{ij})^2
 \end{equation*}
where can rewrite $y_{ij}$ as $\tilde{y}_{ij}$, with $\tilde{y}_{ij} = y_{ij} - \bm{b}^{T}_{i}\bm{z}_{ij}$ which makes SSR:
 \begin{equation*}
  \begin{split}
   SSR& = \sum_{i = 1}^{n}\sum_{j = 1}^{J}( \tilde{y}_{ij}-\bm{\theta}^{T}\bm{x}_{ij})^2\\
   &= ( \tilde{\bm{y}} - \bm{X}\bm{\theta} )^{T}( \tilde{\bm{y}} - \bm{X}\bm{\theta} )\\
   &= \tilde{\bm{y}}^{T}\tilde{\bm{y}} - 2\bm{\theta}^{T}\bm{X}\tilde{\bm{y}} + \bm{\theta}^{T}\bm{X}^{T}\bm{X}\bm{\theta}
  \end{split}
 \end{equation*}
Hence,
 \begin{equation*}
	p(\bm{y}|\bm{\theta}, \bm{X}, \bm{Z}, \bm{\Psi}, \sigma^2) \propto exp(-\frac{1}{2\sigma^2}[- 2\bm{\theta}^{T}\bm{X}\tilde{\bm{y}} + \bm{\theta}^{T}\bm{X}^{T}\bm{X}\bm{\theta}])
 \end{equation*}
Combining this with the prior we obtain:
 \begin{equation}
  \begin{split}
	p(\bm{\theta}|\bm{y}, \bm{X}, \bm{Z}, \bm{\Psi}, \sigma^2)& \propto exp(-\frac{1}{2}\bm{\theta}^{T}\bm{X}^{T}\bm{X}\bm{\theta} + \bm{\theta}^{T}\bm{X}\tilde{\bm{y}}) \\
	\bm{\theta}|.&\sim \bm{N}\left(\frac{(\bm{X}^{T}\bm{X})^{-1}\bm{X}\bm{\tilde{y}}}{\sigma^2}, \frac{(\bm{X}^{T}\bm{X})^{-1}}{\sigma^2}\right)
  \end{split}
 \end{equation}

\paragraph{Full conditional for $\bm{b}_{i}$} (random effects)\\

To derive this one we can start from:
 \begin{equation*}
	p(\bm{b}_{i}|\bm{y}, \bm{X}, \bm{Z}, \bm{\theta}, \bm{\Psi}, \sigma^2) = 	p(\bm{y_{i}}|\bm{\theta}, \bm{b}_{i}, \bm{X}, \bm{Z}, \bm{\Psi}, \sigma^2) p(\bm{b}_{i})
 \end{equation*}
We know that
 \begin{equation*}
	p(\bm{y}_{i}|.) = \prod_{j=1}^{J}p(y_{ij}|\bm{\theta}^{T}\bm{x}_{ij} + \bm{b}_{i}^{T}\bm{z}_{ij}, \sigma^2) \propto exp(-\frac{1}{2\sigma^2}SSR_{i})
 \end{equation*}
with
 \begin{equation*}
	SSR = \sum_{j = 1}^{J}( y_{ij}-\bm{\theta}^{T}\bm{x}_{ij} - \bm{b}_{i}^{T}\bm{z}_{ij})^2
 \end{equation*}
and we can rewrite $y_{ij}$ as $\tilde{y}_{ij} = y_{ij} - \bm{\theta}^{T}\bm{x}_{ij}$, which would make SSR be\\
 \begin{equation*}
  \begin{split}
	SSR& = \sum_{j = 1}^{J}( \tilde{y}_{j}-\bm{\theta}^{T}\bm{x}_{j})^2\\
	&= ( \tilde{\bm{y}} - \bm{b}_{i}^{T}\bm{Z}_{i})^{T}( \tilde{\bm{y}} - \bm{b}_{i}^{T}\bm{Z}_{i})\\
	&= \tilde{\bm{y}}^{T}\tilde{\bm{y}} - 2\bm{b}_{i}^{T}\bm{Z}_{j}\tilde{\bm{y}} + \bm{b}_{i}^{T}\bm{Z}^{T}_{i} \bm{Z}_{j}\bm{b}_{i}
  \end{split}
 \end{equation*}
Hence,
 \begin{equation*}
	p(\bm{y}_{i}|.) \propto exp(-\frac{1}{2\sigma^2}[- 2\bm{b}_{i}^{T}\bm{Z}_{j}\tilde{\bm{y}} + \bm{b}_{i}^{T}\bm{Z}^{T}_{i} \bm{Z}_{j}\bm{b}_{i}])
 \end{equation*}
We also know that in this case, the "prior" is
 \begin{equation*}
	p(\bm{b}_{i}) \propto N(\bm{0}, \bm{\Psi}) \propto exp(-\frac{1}{2}[-2\bm{b}_{i}^{T}\bm{\Psi}^{-1}\bm{0} + \bm{b}_{i}^{T}\bm{\Psi}^{-1}\bm{b}_{i}])
 \end{equation*}
In conclusion, combining the sampling model and the prior, we get:
 \begin{equation}
  \begin{split}
	p(\bm{b}_{i}|.)& \propto 	
	exp(
	-\frac{1}{2\sigma^2}[- 2\bm{b}_{i}^{T}\bm{Z}_{j}\tilde{\bm{y}} + \bm{b}_{i}^{T}\bm{Z}^{T}_{i} \bm{Z}_{j}\bm{b}_{i}]
	-\frac{1}{2\sigma^2}[- 2\bm{b}_{i}^{T}\bm{Z}_{j}\tilde{\bm{y}} + \bm{b}_{i}^{T}\bm{Z}^{T}_{i} \bm{Z}_{j}\bm{b}_{i}])\\
	\bm{b}_{i}|.& \propto \bm{N}\left(\left(\Psi^{-1} + \frac{\bm{Z}_{i}^{T}\bm{Z}_{i}}{\sigma^2}\right)^{-1}\left(\bm{\Psi}^{-1}\bm{0}+\frac{\bm{Z}_{i}^{T}\tilde{y}_{i}}{\sigma^2}\right), \left(\Psi^{-1} + \frac{\bm{Z}_{i}^{T}\bm{Z}_{i}}{\sigma^2}\right)^{-1}\right)
  \end{split}
 \end{equation}

\paragraph{Full conditional for $\sigma^2$}(error variance)\\

The full conditional posterior can be expressed as:
 \begin{equation*}
	p(\sigma^2|\bm{y}, \bm{X}, \bm{Z}, \bm{\theta}, \bm{b}_{i}, \bm{\Psi}) = 	p(\bm{y}|\bm{\theta}, \bm{b}_{i}, \bm{X}, \bm{Z}, \bm{\Psi}, \sigma^2) p(\sigma^2)
 \end{equation*}
The sampling model is the same we saw for the full conditional distribution of $\bm{\theta}$:
 \begin{equation*}
  \begin{split}
	p(\bm{y}|\bm{\theta}, \bm{X}, \bm{Z}, \bm{\Psi}, \sigma^2)& = \prod_{i=1}^n \prod_{j=1}^Jp(y_{ij}|\bm{\theta}^{T}\bm{x}_{ij} + \bm{b}_{i}^{T}\bm{z}_{ij}, \bm{\Psi}, \sigma^2)\\
	&= \prod_{i=1}^{n} \prod_{j=1}^{J}(2\pi\sigma^{-2})^{-\frac{1}{2}}exp(-\frac{(y_{ij} - \bm{\theta}^{T}\bm{x}_{ij} - \bm{b}_{i}^{T}\bm{z}_{ij})^2}{2\sigma^2})	
  \end{split}
 \end{equation*}
However, we are now interested in $\sigma^2$, hence
 \begin{equation*}
  \begin{split}
	p(\bm{y}|\bm{\theta},\bm{X},\bm{Z},\bm{\Psi},\sigma^2)& \propto (\sigma^{2})^{-\frac{N}{2}}exp(-\frac{\sum_{i = 1}^{n}\sum_{j = 1}^{J}( y_{ij}-\bm{\theta}^{T}\bm{x}_{ij} - \bm{b}_{i}^{T}\bm{z}_{ij})^2 }{2\sigma^2}) \\
	&\propto (\sigma^{2})^{-\frac{N}{2}}exp(-\frac{1}{2\sigma^2}SSR)
  \end{split}
 \end{equation*}
where $N = \sum_{i}^{n}nj_{i}$ is the entire sample size (all observations within all clusters).
The prior for $\sigma$ is given above, and therefore we can write the full conditional posterior as:
 \begin{equation}
  \begin{split}
	p(\sigma^2|\bm{y},\bm{X},\bm{Z},\bm{\theta},\bm{b}_{i},\bm{\Psi})& \propto (\sigma^{2})^{-\frac{N}{2}-1}exp(-\frac{1}{2\sigma^2}SSR)\\
	\sigma^2|.& \sim IG(\frac{N}{2}, \frac{SSR}{2})
  \end{split}
 \end{equation}

\paragraph{Full conditional for $\bm{\Psi}$}(random effects variance covariance matrix)\\

Here, we need to write down the posteriors for the different priors we specified. First, let us define the sampling model for the random effects.
 \begin{equation}
  \begin{split} 
   \begin{bmatrix} 
	   b_{0i}\\ 
	   b_{1i}
   \end{bmatrix} 
  	= \bm{b}_{i}& \sim N(\bm{0}, \bm{\Psi}) \\
 	p(\bm{b}_{1}, \bm{b}_{2}| \bm{\Psi})& \propto |\bm{\Psi}|^{-\frac{n}{2}} exp\left(-\frac{1}{2}tr(\bm{S}_b\bm{\Psi}^{-1})\right)  
  \end{split} 
 \end{equation}
where $\bm{S}_b$ is $\Sigma_i\bm{b}_i\bm{b}_i^{T}$
\begin{itemize}
	\item given the inverse-Wishart prior

\begin{equation*}
 \begin{split} 
  p(\bm{\Psi})& \propto IW(\nu_0, \bm{S}_{0}) \\
  & \propto |\bm{\Psi}|^{-\frac{(\nu_0 + k + 1)}{2}}exp\left(-\frac{1}{2}tr(\bm{S}_0\bm{\Psi}^{-1}) \right)
 \end{split}
\end{equation*}

the full conditional posterior of $\bm{\Psi}$ is

\begin{equation}
 \begin{split} 
  p(\bm{\Psi}|.)& \propto |\bm{\Psi}|^{-\frac{(\nu_0 + k + 1 + n)}{2}}exp\left(-\frac{1}{2}tr([\bm{S}_0+\bm{S}_b]\bm{\Psi}^{-1}) \right) \\
  & \propto IW(\nu_0 + n, \bm{S}_{0} + \bm{S}_{b})
 \end{split}
\end{equation}	

where $\nu_0 = 2$

	\item inverse-Wishart \textit{a là} Huang and Wand
	
\begin{equation*}
	\begin{split}
	p(\bm{\Psi}|a_1, a_2)& \propto IW(\nu_0 + k - 1, 2\nu_0 diag(1/a_1, 1/a_2)), \\
	a_k& \propto IG(\eta,1/A_k^2) \\
	p(\bm{\Psi})& \propto |\bm{\Psi}|^{-\frac{(\nu_0+k-1+1)}{2}}exp\left(-\frac{1}{2}tr(2\nu_0 diag(1/a_1, 1/a_2)\bm{\Psi}^{-1}) \right) \\
	& \times \left(\frac{1}{a_1}\right)^{\eta+1}exp\left(-\frac{1}{A_1^2a_1}\right) \times \left(\frac{1}{a_2}\right)^{\eta+1}exp\left(-\frac{1}{A_2^2a_2}\right) \\
	\end{split}
\end{equation*}

the full conditional posterior of $\bm{\Psi}$ is

\begin{equation}
 \begin{split} 
  p(\bm{\Psi}|.)& \propto |\bm{\Psi}|^{-\frac{(\nu_0+k-1+n+1)}{2}}exp\left(-\frac{1}{2}tr([\bm{S}_b+2\nu_0 diag(1/a_1, 1/a_2)]\bm{\Psi}^{-1}) \right) \\
  & \propto IW(\nu_0+k-1+n, \bm{S}_{b}+2\nu_0 diag(1/a_1, 1/a_2))\\
  p(a_k|.)& \propto IG\left(\eta(\nu_0+k), \nu_0\left(\bm{\Psi}^{-1}_{kk}+\frac{1}{A_k^2}\right)\right)
 \end{split}
\end{equation}		

where $\eta = \frac{1}{2}, \nu_0 = 2, k = 2$, and $n$ is the number of clusters (individuals). (For the conditional psoterior of $a_k$ refer to Huang and Wand (2013), section 4.2).
	
	\item Matrix-F variate
	
\begin{equation*}
 \begin{split} 
  p(\bm{\Psi})& \propto F(\bm{\Psi}; \nu_0, \delta, \bm{B}) \\
  & \propto \int IW(\bm{\Psi}; \delta + k - 1, \bm{\Omega}) \times W(\bm{\Omega}; \nu_0, \bm{B})d\bm{\Omega} \\
  & \propto \int |\bm{\Psi}|^{-\frac{(\delta+2k)}{2}}exp\left(-\frac{1}{2}tr(\bm{\Omega}\bm{\Psi}^{-1})\right) \times |\bm{\Omega}|^{-\frac{(\nu-k-1)}{2}}exp\left(-\frac{1}{2}tr(\bm{\Omega}\bm{B}^{-1})\right)d\bm{\Omega}
 \end{split}
\end{equation*}	

the full conditional posterior of $\bm{\Psi}$ is

\begin{equation}
 \begin{split} 
  p(\bm{\Psi}|.)& \propto \int |\bm{\Psi}|^{-\frac{(\delta+2k+n)}{2}}exp\left(-\frac{1}{2}tr([\bm{\Omega}+\bm{S}_b]\bm{\Psi}^{-1})\right) \times |\bm{\Omega}|^{-\frac{(\nu-k-1)}{2}}exp\left(-\frac{1}{2}tr(\bm{\Omega}\bm{B}^{-1})\right)d\bm{\Omega} \\
  & \propto \int IW(\delta+k+n-1, \bm{\Omega} + \bm{S}_{b}) \times W(\nu, \bm{B})d\bm{\Omega}
 \end{split}
\end{equation}			

with $\nu = k = 2, \delta = 1$ and $\bm{B}$ prior guess
	
\end{itemize}

\paragraph{Notation Conventions}

\begin{itemize}
	\item $n$ number of clusters; $i$ specific cluster
	\item $J$ number of observations within cluster; $j$ specific observation
	\item $N$ total number of observations
\end{itemize}



\end{document}


