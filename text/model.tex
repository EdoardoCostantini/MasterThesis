%        File: model.tex

\documentclass[a4paper]{article}

% Packages
\usepackage{bm} % for math bold
\usepackage{amsmath} % for the allignment of equations under same number
\usepackage{amsmath}					 % matrix

\begin{document}
Here I define the Bayesian model I want to use to test the performance of the different priors. The features I want to include are the folowing: continuous outcome, any number/measurement scale.

Let's start with the \textbf{model}

\begin{equation}
	y_{ij} = \bm{x}^{T}_{ij} \bm{\theta} + \bm{z}^{T}_{ij}\bm{b}_i + \epsilon_{ij}
\end{equation}

$ \bm{b}_i \sim N(\bm{0}, \bm{\Psi}) $

$ \epsilon_{ij} \sim N(0, \sigma^2) $

\vspace{5mm}

(Vectors are in bold, matrix are capital Greek letters).\\
I'm going to define the following \textbf{priors}:

\vspace{5mm}

\begin{equation}
p(\bm{\theta}) \propto 1
\end{equation}

\begin{equation}
p(\sigma^2) \propto \sigma^{-2}
\end{equation}

For what concerns the random effects variance covariance matrix, different priors are tested. In particular we used:

\begin{itemize}
	\item inverse-Wishart
	
\begin{equation}
p(\bm{\Psi}) \propto IW(\nu_0, S_{0})
\end{equation}	

where we choose $\nu_0$ to be 1 and $S_{0}$ to be diag(2), following indications by Gelman \textit{et al} (2014).

	\item inverse-Wishart \textit{a là} Huang and Wand

\begin{equation}
	\begin{split}
	p(\bm{\Psi|a_1, a_2})& \propto IW(\nu_0 + p - 1, 2\nu_0 diag(1/a_1, 1/a_2)), \\
	a_k& \propto IG(1/2,1/A_k^2),
	\end{split}
\end{equation}

with $\nu_0 = 2$ and $\bm{A} = [100, 100]$. Considering a $2 \times 2$ random effects variance covariance matrix (random intercepts, and random slopes) that has this prior distribution, the corresponding marginal distributions of the standard deviation random components have an half-Cauchy distribution (belonging to the half-\textit{t} family, see Gelman (2006) for details). In particular, any square-root diagonal element $\sigma_k$ of a matrix following such a distribution is Half-\textit{t}$(\nu_0, A_k)$. When, $\nu = 1$, and $A_k$ is equal to the prior guess we have an half-Cauchy prior distribution for a $\sigma_k$ parameter. Furthermore, the marginal priors for the correlations have a beta distribution with parameters $\alpha = -1, \beta = -1$ (Huang and Wand, 2013).
	
	\item Matrix-F variate
	
\begin{equation}
\begin{split}
p(\bm{\Psi})& \propto F(\bm{\Psi}; \nu_0, \delta, \bm{B}) \\
& \propto \int IW(\bm{\Psi}; \delta + k - 1, \Sigma) \times W(\bm{\Sigma}; \nu_0, \bm{B})d\bm{\Sigma}
\end{split}
\end{equation}	
\end{itemize}

where $\nu_0 = 2$, $\delta = 1$, and $\bm{B}$ is a prior guess. Three different choices where made for $\bm{B}$ in this paper: diag($10^3$), proper neighbor of $(\sigma^2)^{-\frac{1}{2}}$; $\bm{B}_{ed}$, an educated guess based on data exploration, $\bm{R^*}$ and an empirical bayes choice following Kass and Natarajan (2006). \\
Considering a $2 \times 2$ random effects variance covariance matrix (random intercepts, and random slopes) that is matrix-F distirbuted, $F(\nu_0, \delta, \bm{B})$, the marginal distribution on the standard deviations of the random effects are univariate $F(\nu_0, \delta, b_{11})$ and $F(\nu_0, \delta, b_{22})$, with $\nu_0 >1, \delta > 0, b_{jj} > 0$.

\vspace{5mm}

The derivation of the conditional posterior follows.

\paragraph{Full conditional for $\bm{\theta}$}(fixed effects) \\

Let's start with

\begin{equation}
	p(\bm{\theta}|\bm{y}, \bm{X}, \bm{Z}, \bm{b}_{i}, \bm{\Psi}, \sigma^2) = 	p(\bm{y}|\bm{\theta}, \bm{X}, \bm{Z}, \bm{b}_{i}, \bm{\Psi}, \sigma^2) p(\bm{\theta})
\end{equation}

where\\

$p(\bm{y}|\bm{\theta}, \bm{X}, \bm{Z}, \bm{\Psi}, \sigma^2) = \prod_{i=1}^n \prod_{j=1}^Jp(y_{ij}|\bm{\theta}^{T}\bm{x}_{ij} + \bm{b}_{i}^{T}\bm{z}_{ij}, \bm{\Psi}, \sigma^2) \propto exp(-\frac{1}{2\sigma^2}SSR)$ \\

and\\

$SSR = \sum_{i = 1}^{n}[\sum_{j = 1}^{J}( y_{ij}-\bm{\theta}^{T}\bm{x}_{ij} - \bm{b}_{i}^{T}\bm{z}_{ij})^2]$

where can rewrite $y_{ij}$ as $\tilde{y}_{ij}$, with $\tilde{y}_{ij} = y_{ij} - \bm{b}^{T}_{i}\bm{z}_{ij}$. This would turn SSR in:\\

$SSR = \sum_{i = 1}^{n}[\sum_{j = 1}^{J}( \tilde{y}_{ij}-\bm{\theta}^{T}\bm{x}_{ij})^2 ] =$

$= ( \tilde{\bm{y}} - \bm{X}\bm{\theta} )^{T}( \tilde{\bm{y}} - \bm{X}\bm{\theta} )$

$ = \tilde{\bm{y}}^{T}\tilde{\bm{y}} - 2\bm{\theta}^{T}\bm{X}\tilde{\bm{y}} + \bm{\theta}^{T}\bm{X}^{T}\bm{X}\bm{\theta}$\\

Hence,
\begin{equation}
p(\bm{y}|\bm{\theta}, \bm{X}, \bm{Z}, \bm{\Psi}, \sigma^2) \propto exp(-\frac{1}{2\sigma^2}[- 2\bm{\theta}^{T}\bm{X}\tilde{\bm{y}} + \bm{\theta}^{T}\bm{X}^{T}\bm{X}\bm{\theta}])
\end{equation}


Combining this with the prior we obtain:

\begin{equation}
	p(\bm{\theta}|\bm{y}, \bm{X}, \bm{Z}, \bm{\Psi}, \sigma^2) \propto exp(-\frac{1}{2}\bm{\theta}^{T}\bm{X}^{T}\bm{X}\bm{\theta} + \bm{\theta}^{T}\bm{X}\tilde{\bm{y}})
\end{equation}

\begin{equation}
	p(\bm{\theta}|\bm{y}, \bm{X}, \bm{Z}, \bm{\Psi}, \sigma^2) \sim multivariate-N(\frac{(\bm{X}^{T}\bm{X})^{-1}\bm{X}\bm{\tilde{y}}}{\sigma^2}, \frac{(\bm{X}^{T}\bm{X})^{-1}}{\sigma^2})
\end{equation}

\paragraph{Full conditional for $\bm{b}_{i}$} (random effects)

To derive this one we can start from:

\begin{equation}
	p(\bm{b}_{i}|\bm{y}, \bm{X}, \bm{Z}, \bm{\theta}, \bm{\Psi}, \sigma^2) = 	p(\bm{y_{i}}|\bm{\theta}, \bm{b}_{i}, \bm{X}, \bm{Z}, \bm{\Psi}, \sigma^2) p(\bm{b}_{i})
\end{equation}

We know that \\

$p(\bm{y}_{i}|.) = \prod_{j=1}^{J}p(y_{ij}|\bm{\theta}^{T}\bm{x}_{ij} + \bm{b}_{i}^{T}\bm{z}_{ij}, \sigma^2) \propto exp(-\frac{1}{2\sigma^2}SSR_{i})$\\

with\\

$SSR = \sum_{j = 1}^{J}( y_{ij}-\bm{\theta}^{T}\bm{x}_{ij} - \bm{b}_{i}^{T}\bm{z}_{ij})^2$

and we can rewrite $y_{ij}$ as $\tilde{y}_{ij} = y_{ij} - \bm{\theta}^{T}\bm{x}_{ij}$, which would make SSR be\\

$SSR = \sum_{j = 1}^{J}( \tilde{y}_{j}-\bm{\theta}^{T}\bm{x}_{j})^2=$

$= ( \tilde{\bm{y}} - \bm{b}_{i}^{T}\bm{Z}_{i})^{T}( \tilde{\bm{y}} - \bm{b}_{i}^{T}\bm{Z}_{i})$

$ = \tilde{\bm{y}}^{T}\tilde{\bm{y}} - 2\bm{b}_{i}^{T}\bm{Z}_{j}\tilde{\bm{y}} + \bm{b}_{i}^{T}\bm{Z}^{T}_{i} \bm{Z}_{j}\bm{b}_{i}$\\

Hence,
\begin{equation}
	p(\bm{y}_{i}|.) \propto exp(-\frac{1}{2\sigma^2}[- 2\bm{b}_{i}^{T}\bm{Z}_{j}\tilde{\bm{y}} + \bm{b}_{i}^{T}\bm{Z}^{T}_{i} \bm{Z}_{j}\bm{b}_{i}])
\end{equation}

We also know that in this case, the "prior" is

\begin{equation}
p(\bm{b}_{i}) \propto N(\bm{0}, \bm{\Psi}) \propto exp(-\frac{1}{2}[-2\bm{b}_{i}^{T}\bm{\Psi}^{-1}\bm{0} + \bm{b}_{i}^{T}\bm{\Psi}^{-1}\bm{b}_{i}])
\end{equation}

In conclusion, combining the sampling model and the prior, we get:

\begin{equation}
	p(\bm{b}_{i}|.) \propto 	
	exp(
	-\frac{1}{2\sigma^2}[- 2\bm{b}_{i}^{T}\bm{Z}_{j}\tilde{\bm{y}} + \bm{b}_{i}^{T}\bm{Z}^{T}_{i} \bm{Z}_{j}\bm{b}_{i}]
	-\frac{1}{2\sigma^2}[- 2\bm{b}_{i}^{T}\bm{Z}_{j}\tilde{\bm{y}} + \bm{b}_{i}^{T}\bm{Z}^{T}_{i} \bm{Z}_{j}\bm{b}_{i}]) 
\end{equation}

\begin{equation}
	p(\bm{b}_{i}|.) 	\propto 	multiv-N((\Psi^{-1} + \frac{\bm{Z}_{i}^{T}\bm{Z}_{i}}{\sigma^2})^{-1}(\bm{\Psi}^{-1}\bm{0}+\frac{\bm{Z}_{i}^{T}\tilde{y}_{i}}{\sigma^2}), (\Psi^{-1} + \frac{\bm{Z}_{i}^{T}\bm{Z}_{i}}{\sigma^2})^{-1})	
\end{equation}


\paragraph{Full conditional for $\sigma^2$}(error variance)
The full conditional posterior can be expressed as:

\begin{equation}
	p(\sigma^2|\bm{y}, \bm{X}, \bm{Z}, \bm{\theta}, \bm{b}_{i}, \bm{\Psi}) = 	p(\bm{y}|\bm{\theta}, \bm{b}_{i}, \bm{X}, \bm{Z}, \bm{\Psi}, \sigma^2) p(\sigma^2)
\end{equation}

The sampling model is the same we saw for the full conditional distribution of $\bm{\theta}$:\\

$p(\bm{y}|\bm{\theta}, \bm{X}, \bm{Z}, \bm{\Psi}, \sigma^2) = \prod_{i=1}^n \prod_{j=1}^Jp(y_{ij}|\bm{\theta}^{T}\bm{x}_{ij} + \bm{b}_{i}^{T}\bm{z}_{ij}, \bm{\Psi}, \sigma^2)=$

$ = \prod_{i=1}^{n} \prod_{j=1}^{J}(2\pi\sigma^{-2})^{-\frac{1}{2}}exp(-\frac{(y_{ij} - \bm{\theta}^{T}\bm{x}_{ij} - \bm{b}_{i}^{T}\bm{z}_{ij})^2}{2\sigma^2})$\\

However, we are now interested in $\sigma^2$, hence\\

$p(\bm{y}|\bm{\theta}, \bm{X}, \bm{Z}, \bm{\Psi}, \sigma^2) \propto (\sigma^{2})^{-\frac{N}{2}}exp(-\frac{\sum_{i = 1}^{n}\sum_{j = 1}^{J}( y_{ij}-\bm{\theta}^{T}\bm{x}_{ij} - \bm{b}_{i}^{T}\bm{z}_{ij})^2 }{2\sigma^2})$
$=(\sigma^{2})^{-\frac{N}{2}}exp(-\frac{1}{2\sigma^2}SSR)$ \\

where $N = \sum_{i}^{n}nj_{i}$ is the entire sample size (all observations within all clusters).\\

The prior for $\sigma$ is given above, and therefore we can write the full conditional posterior as:

\begin{equation}
	p(\sigma^2|\bm{y}, \bm{X}, \bm{Z}, \bm{\theta}, \bm{b}_{i}, \bm{\Psi}) \propto (\sigma^{2})^{-\frac{N}{2}-1}exp(-\frac{1}{2\sigma^2}SSR)
\end{equation}

\begin{equation}
	p(\sigma^2|\bm{y}, \bm{X}, \bm{Z}, \bm{\theta}, \bm{b}_{i}, \bm{\Psi}) \sim IG(\frac{N}{2}, \frac{SSR}{2})
\end{equation}



\paragraph{Full conditional for $\bm{\Psi}$}(random effects variance covariance matrix)\\

Here, we need to write down the posteriors for the different priors we specified. First, let us define the sampling model for the random effects.

$M = \begin{smallmatrix} a&b\\ c&d \end{smallmatrix}$

\begin{equation}
\begin{split}
\begin{bmatrix} 
  b_{0i}\\ 
  b_{1i}
\end{bmatrix} 
= \bm{b}_{i}& \sim N(\bm{0}, \bm{\Psi}) \\
p(\bm{b}_{1}, \bm{b}_{2}| \bm{\Psi})& \propto |\bm{\Psi}|^{-\frac{n}{2}}
\end{split}
\end{equation}

\begin{itemize}
	\item inverse-Wishart
	
	\item inverse-Wishart \textit{a là} Huang and Wand
	
	\item Matrix-F variate
\end{itemize}

See Mulder Pericchi, 2018

\paragraph{Notation Conventions}

\begin{itemize}
	\item $n$ number of clusters; $i$ specific cluster
	\item $J$ number of observations within cluster; $j$ specific observation
	\item $N$ total number of observations
\end{itemize}



\end{document}


