%        File: model.tex

\documentclass[a4paper]{article}

% Packages
\usepackage{bm} % for math bold
\usepackage{amsmath} % for the allignment of equations under same number
\usepackage{amsmath}					 % matrix
\usepackage[a4paper, total={17cm, 23cm}]{geometry} %a4 = 21cm x 29.7cm

\begin{document}
A fundamental issue of Bayesian Statistics is the definition uninformative priors. It is common to distinguish between objective and subjective Bayesian approaches to the definition of priors (elaborate more). As Bayesian approaches to data analysis become more and more widespread a need for standard ways of defining priors for analyses becomes more and more relevant. While this issue might be easier for some types of analysis (elaborate more), when models become more complicated the issue becomes non-trivial. In particular there is no standard way of choosing an uninformative prior for the random effects variance-covariance matrix parameter in mixed effects models.
Publications have flourished on the matter but have often focused on the simpler case of a scalar random effect, the random intercept model. Here I will explain Brown and Draper 2006, Gelman 2006, Kass and Natarajan 2006 solutions.
Some publications extended the reasoning to the vector random effects case, elaborating on the definition of a prior distribution for the variance covariance matrix (henceforth referred to as $\bm{\Psi}$), as random intercept and random slopes models.

Here I define the Bayesian model I want to use to test the performance of the different priors. The features I want to include are the folowing: continuous outcome, any number/measurement scale.

Let's start with the \textbf{model}

\begin{equation}
 \begin{split}
	y_{ij}& = \bm{x}^{T}_{ij} \bm{\theta} + \bm{z}^{T}_{ij}\bm{b}_i + \epsilon_{ij}\\
	\bm{b}_i& \sim N(\bm{0}, \bm{\Psi})\\
	\epsilon_{ij}& \sim N(0, \sigma^2)
 \end{split}
\end{equation}

(Vectors are in bold, matrix are capital Greek letters).\\
I'm going to define the following \textbf{priors}:

\begin{equation}
p(\bm{\theta}) \propto 1
\end{equation}
\begin{equation}
p(\sigma^2) \propto \sigma^{-2}
\end{equation}

For what concerns the random effects variance covariance matrix, different priors are tested. In particular we used:

\begin{itemize}
	\item inverse-Wishart
	
\begin{equation}
p(\bm{\Psi}) \propto IW(\nu, S_{0})
\end{equation}	

where we choose $\nu = k-1+e$, and $S_{0}=diag(k-1+e)$, following indications by Gelman \textit{et al} (2014). Given a $\bm{\Psi} \sim IW(\nu, S_{0})$, where $\bm{\Psi}$ and $S_{0}$ are $k \times k$ matrices, it is known that $\bm{\Psi}_{11}$, the $k_1 \times k_1$ upper-left triangular sub-matrix of $\bm{\Psi}$, has an inverse-Wishart distribution as well. In particular, $\bm{\Psi}_{11} \sim IW(\nu-(k-k_1), S_{011})$. Furthermore, for a univariate case ($k=1$), we know that an inverse Wishart distribution simplifies to an inverse Gamma with parameters $\alpha=\frac{\nu}{2}, \beta = \frac{S_{0kk}}{2}$. With the goal of ressempling as close as possible what Gelman 2006 did, we try to define inverse wishart priors for $\bm{\Psi}$, such that the the marginal disitbrution is as close as possible to the IG(e, e) used by Gelman. This goal is achieved by setting $\nu = k-1+e$ and $S_{011} = k-1+e$, which makes the marginal distribution on the variance components (diagonals of $\bm{\Psi}$) $\sigma_{ii}^{2} \sim IG(\frac{\nu-1}{2},\frac{S_{0ii}}{2})$. The inverse-Wishart priors we defined are:
\begin{center}
\begin{tabular}{ c c c }
 Prior Description & $\nu$ & $S_{0}$ \\ 
 \hline
 \multicolumn{1}{l}{1. IW educated} & $2$ & educated guess \\
 \multicolumn{1}{l}{2. IW uninformative} & $k-1+e$ & $(k-1+e-1)\times$ diag(k) \\  
 \hline
\end{tabular}
\end{center}
1, .01, and .001 are then used as values of e.

	\item inverse-Wishart \textit{a là} Huang and Wand

\begin{equation}
	\begin{split}
	p(\bm{\Psi}|a_1, a_2)& \propto IW(\nu + k - 1, 2\nu \times diag(1/a_1, 1/a_2)), \\
	a_k& \propto IG(1/2,1/A_k^2),
	\end{split}
\end{equation}

with $\nu = 2$ and $\bm{A} = [1000, 1000]$. The marginal distribution of any standard deviation term in $\bm{\Psi}$ is Half-\textit{t}$(\nu, A_k)$ and, when choosing $\nu = 2$, the marginal disitbrution on the correlation term is uniform on (-1, 1), see property 2 to 4 in Huang and Wand (2013, p. 442). Furthermore, according to Huang and Wand (2013, p. 441) arbitrarily large values for $a_k$ lead to arbitrarily weak priors on the standard deviation term. Hence, our choices for the parameters of this prior are: 
	
\begin{center}
\begin{tabular}{ c c c }
 Prior Description & $\nu$ & $\bm{A}$ \\ 
 \hline
 \multicolumn{1}{l}{3. IW a là HW} & $2$ & $[1000, 1000]$ \\  
 \hline
\end{tabular}
\end{center}	
	
	\item Matrix-F variate
	
\begin{equation}
\begin{split}
p(\bm{\Psi})& \propto F(\bm{\Psi}; \nu, \delta, \bm{B}) \\
& \propto \int IW(\bm{\Psi}; \delta + k - 1, \Sigma) \times W(\bm{\Sigma}; \nu, \bm{B})d\bm{\Sigma}
\end{split}
\end{equation}	

with degrees of freedom $\nu>k-1$, $\delta>0$, and $\bm{B}$ a positive definite scale matrix that functions as prior guess. Three different choices where made for $\bm{B}$ in this paper: diag($10^3$), proper neighbor of $(\sigma^2)^{-\frac{1}{2}}$; $\bm{B}_{ed}$, an educated guess based on data exploration, $\bm{R^*}$ and an empirical bayes choice following Kass and Natarajan (2006).
Considering a $2 \times 2$ random effects variance covariance matrix (random intercepts, and random slopes) that is matrix-F distributed, $F(\nu, \delta, \bm{B})$, the marginal distribution on the standard deviations of the random effects are univariate $F(\nu, \delta, b_{11})$ and $F(\nu, \delta, b_{22})$, with $\nu >1, \delta > 0, b_{jj} > 0$. To achive uninformativity of this prior we defined $\nu = k -1 + \varepsilon $, $\delta = \varepsilon$, $B = S_0$, with $S_0$ some covariance matrix prior guess and $\epsilon$ a small quantity (1, .5, .1). There we chose the first integer number we could for the parameters $\nu$, and $\delta$. Using the matrix-F prior, we specified two further priors: a proper neighborhood of $|\Sigma^\frac{1}{2}|$ following Mulder Pericchi 2018 ($\nu = 2, \bm{B} = \textit{b}_k \times \bm{I}_k$, with $b=10^3$ an arbitrarily large number); and the default conjugate prior proposed by Kass and Natarajan. The following table summarizes the prior decisions.

\begin{center}
\begin{tabular}{ c c c c }
 Prior Description & $\nu$ & $\delta$ & $S_{0}$ \\ 
 \hline
 \multicolumn{1}{l}{4. mat-F proper neighbor} & $2$ & $1$ & $10^3 \times \bm{I}_2$ \\
 \multicolumn{1}{l}{5. mat-F uninformative} & $k-1+\varepsilon$ & $\varepsilon$ & educated guess \\
 \multicolumn{1}{l}{6. mat-F educated guess} & $2$ & $1$ & \textbf{\textit{R}}* \\ 
 \hline
  \multicolumn{1}{l}{$\varepsilon$ is set to each 1, .5, and .1} 
\end{tabular}
\end{center}

\end{itemize}

The derivation of the conditional posterior follows.

\paragraph{Full conditional for $\bm{\theta}$}(fixed effects) \\

Let's start with
 \begin{equation*}
	p(\bm{\theta}|\bm{y}, \bm{X}, \bm{Z}, \bm{b}_{i}, \bm{\Psi}, \sigma^2) = 	p(\bm{y}|\bm{\theta}, \bm{X}, \bm{Z}, \bm{b}_{i}, \bm{\Psi}, \sigma^2) p(\bm{\theta})
 \end{equation*}
where
 \begin{equation*}
  \begin{split}
  p(\bm{y}|\bm{\theta},\bm{X},\bm{Z},\bm{\Psi},\sigma^2) &= \prod_{i=1}^n \prod_{j=1}^Jp(y_{ij}|\bm{\theta}^{T}\bm{x}_{ij} + \bm{b}_{i}^{T}\bm{z}_{ij}, \bm{\Psi}, \sigma^2) \\
  &\propto exp(-\frac{1}{2\sigma^2}SSR)
  \end{split}
 \end{equation*}
and
 \begin{equation*}
  SSR = \sum_{i = 1}^{n}\sum_{j = 1}^{J}( y_{ij}-\bm{\theta}^{T}\bm{x}_{ij} - \bm{b}_{i}^{T}\bm{z}_{ij})^2
 \end{equation*}
where can rewrite $y_{ij}$ as $\tilde{y}_{ij}$, with $\tilde{y}_{ij} = y_{ij} - \bm{b}^{T}_{i}\bm{z}_{ij}$ which makes SSR:
 \begin{equation*}
  \begin{split}
   SSR& = \sum_{i = 1}^{n}\sum_{j = 1}^{J}( \tilde{y}_{ij}-\bm{\theta}^{T}\bm{x}_{ij})^2\\
   &= ( \tilde{\bm{y}} - \bm{X}\bm{\theta} )^{T}( \tilde{\bm{y}} - \bm{X}\bm{\theta} )\\
   &= \tilde{\bm{y}}^{T}\tilde{\bm{y}} - 2\bm{\theta}^{T}\bm{X}\tilde{\bm{y}} + \bm{\theta}^{T}\bm{X}^{T}\bm{X}\bm{\theta}
  \end{split}
 \end{equation*}
Hence,
 \begin{equation*}
	p(\bm{y}|\bm{\theta}, \bm{X}, \bm{Z}, \bm{\Psi}, \sigma^2) \propto exp(-\frac{1}{2\sigma^2}[- 2\bm{\theta}^{T}\bm{X}\tilde{\bm{y}} + \bm{\theta}^{T}\bm{X}^{T}\bm{X}\bm{\theta}])
 \end{equation*}
Combining this with the prior we obtain:
 \begin{equation}
  \begin{split}
	p(\bm{\theta}|\bm{y}, \bm{X}, \bm{Z}, \bm{\Psi}, \sigma^2)& \propto exp(-\frac{1}{2}\bm{\theta}^{T}\bm{X}^{T}\bm{X}\bm{\theta} + \bm{\theta}^{T}\bm{X}\tilde{\bm{y}}) \\
	\bm{\theta}|.&\sim \bm{N}\left(\frac{(\bm{X}^{T}\bm{X})^{-1}\bm{X}\bm{\tilde{y}}}{\sigma^2}, \frac{(\bm{X}^{T}\bm{X})^{-1}}{\sigma^2}\right)
  \end{split}
 \end{equation}

\paragraph{Full conditional for $\bm{b}_{i}$} (random effects)\\

To derive this one we can start from:
 \begin{equation*}
	p(\bm{b}_{i}|\bm{y}, \bm{X}, \bm{Z}, \bm{\theta}, \bm{\Psi}, \sigma^2) = 	p(\bm{y_{i}}|\bm{\theta}, \bm{b}_{i}, \bm{X}, \bm{Z}, \bm{\Psi}, \sigma^2) p(\bm{b}_{i})
 \end{equation*}
We know that
 \begin{equation*}
	p(\bm{y}_{i}|.) = \prod_{j=1}^{J}p(y_{ij}|\bm{\theta}^{T}\bm{x}_{ij} + \bm{b}_{i}^{T}\bm{z}_{ij}, \sigma^2) \propto exp(-\frac{1}{2\sigma^2}SSR_{i})
 \end{equation*}
with
 \begin{equation*}
	SSR = \sum_{j = 1}^{J}( y_{ij}-\bm{\theta}^{T}\bm{x}_{ij} - \bm{b}_{i}^{T}\bm{z}_{ij})^2
 \end{equation*}
and we can rewrite $y_{ij}$ as $\tilde{y}_{ij} = y_{ij} - \bm{\theta}^{T}\bm{x}_{ij}$, which would make SSR be\\
 \begin{equation*}
  \begin{split}
	SSR& = \sum_{j = 1}^{J}( \tilde{y}_{j}-\bm{\theta}^{T}\bm{x}_{j})^2\\
	&= ( \tilde{\bm{y}} - \bm{b}_{i}^{T}\bm{Z}_{i})^{T}( \tilde{\bm{y}} - \bm{b}_{i}^{T}\bm{Z}_{i})\\
	&= \tilde{\bm{y}}^{T}\tilde{\bm{y}} - 2\bm{b}_{i}^{T}\bm{Z}_{j}\tilde{\bm{y}} + \bm{b}_{i}^{T}\bm{Z}^{T}_{i} \bm{Z}_{j}\bm{b}_{i}
  \end{split}
 \end{equation*}
Hence,
 \begin{equation*}
	p(\bm{y}_{i}|.) \propto exp(-\frac{1}{2\sigma^2}[- 2\bm{b}_{i}^{T}\bm{Z}_{j}\tilde{\bm{y}} + \bm{b}_{i}^{T}\bm{Z}^{T}_{i} \bm{Z}_{j}\bm{b}_{i}])
 \end{equation*}
We also know that in this case, the "prior" is
 \begin{equation*}
	p(\bm{b}_{i}) \propto N(\bm{0}, \bm{\Psi}) \propto exp(-\frac{1}{2}[-2\bm{b}_{i}^{T}\bm{\Psi}^{-1}\bm{0} + \bm{b}_{i}^{T}\bm{\Psi}^{-1}\bm{b}_{i}])
 \end{equation*}
In conclusion, combining the sampling model and the prior, we get:
 \begin{equation}
  \begin{split}
	p(\bm{b}_{i}|.)& \propto 	
	exp(
	-\frac{1}{2\sigma^2}[- 2\bm{b}_{i}^{T}\bm{Z}_{j}\tilde{\bm{y}} + \bm{b}_{i}^{T}\bm{Z}^{T}_{i} \bm{Z}_{j}\bm{b}_{i}]
	-\frac{1}{2\sigma^2}[- 2\bm{b}_{i}^{T}\bm{Z}_{j}\tilde{\bm{y}} + \bm{b}_{i}^{T}\bm{Z}^{T}_{i} \bm{Z}_{j}\bm{b}_{i}])\\
	\bm{b}_{i}|.& \propto \bm{N}\left(\left(\Psi^{-1} + \frac{\bm{Z}_{i}^{T}\bm{Z}_{i}}{\sigma^2}\right)^{-1}\left(\bm{\Psi}^{-1}\bm{0}+\frac{\bm{Z}_{i}^{T}\tilde{y}_{i}}{\sigma^2}\right), \left(\Psi^{-1} + \frac{\bm{Z}_{i}^{T}\bm{Z}_{i}}{\sigma^2}\right)^{-1}\right)
  \end{split}
 \end{equation}

\paragraph{Full conditional for $\sigma^2$}(error variance)\\

The full conditional posterior can be expressed as:
 \begin{equation*}
	p(\sigma^2|\bm{y}, \bm{X}, \bm{Z}, \bm{\theta}, \bm{b}_{i}, \bm{\Psi}) = 	p(\bm{y}|\bm{\theta}, \bm{b}_{i}, \bm{X}, \bm{Z}, \bm{\Psi}, \sigma^2) p(\sigma^2)
 \end{equation*}
The sampling model is the same we saw for the full conditional distribution of $\bm{\theta}$:
 \begin{equation*}
  \begin{split}
	p(\bm{y}|\bm{\theta}, \bm{X}, \bm{Z}, \bm{\Psi}, \sigma^2)& = \prod_{i=1}^n \prod_{j=1}^Jp(y_{ij}|\bm{\theta}^{T}\bm{x}_{ij} + \bm{b}_{i}^{T}\bm{z}_{ij}, \bm{\Psi}, \sigma^2)\\
	&= \prod_{i=1}^{n} \prod_{j=1}^{J}(2\pi\sigma^{-2})^{-\frac{1}{2}}exp(-\frac{(y_{ij} - \bm{\theta}^{T}\bm{x}_{ij} - \bm{b}_{i}^{T}\bm{z}_{ij})^2}{2\sigma^2})	
  \end{split}
 \end{equation*}
However, we are now interested in $\sigma^2$, hence
 \begin{equation*}
  \begin{split}
	p(\bm{y}|\bm{\theta},\bm{X},\bm{Z},\bm{\Psi},\sigma^2)& \propto (\sigma^{2})^{-\frac{N}{2}}exp(-\frac{\sum_{i = 1}^{n}\sum_{j = 1}^{J}( y_{ij}-\bm{\theta}^{T}\bm{x}_{ij} - \bm{b}_{i}^{T}\bm{z}_{ij})^2 }{2\sigma^2}) \\
	&\propto (\sigma^{2})^{-\frac{N}{2}}exp(-\frac{1}{2\sigma^2}SSR)
  \end{split}
 \end{equation*}
where $N = \sum_{i}^{n}nj_{i}$ is the entire sample size (all observations within all clusters).
The prior for $\sigma$ is given above, and therefore we can write the full conditional posterior as:
 \begin{equation}
  \begin{split}
	p(\sigma^2|\bm{y},\bm{X},\bm{Z},\bm{\theta},\bm{b}_{i},\bm{\Psi})& \propto (\sigma^{2})^{-\frac{N}{2}-1}exp(-\frac{1}{2\sigma^2}SSR)\\
	\sigma^2|.& \sim IG(\frac{N}{2}, \frac{SSR}{2})
  \end{split}
 \end{equation}

\paragraph{Full conditional for $\bm{\Psi}$}(random effects variance covariance matrix)\\

Here, we need to write down the posteriors for the different priors we specified. First, let us define the sampling model for the random effects.
 \begin{equation}
  \begin{split} 
   \begin{bmatrix} 
	   b_{0i}\\ 
	   b_{1i}
   \end{bmatrix} 
  	= \bm{b}_{i}& \sim N(\bm{0}, \bm{\Psi}) \\
 	p(\bm{b}_{1}, \bm{b}_{2}| \bm{\Psi})& \propto |\bm{\Psi}|^{-\frac{n}{2}} exp\left(-\frac{1}{2}tr(\bm{S}_b\bm{\Psi}^{-1})\right)  
  \end{split} 
 \end{equation}
where $\bm{S}_b$ is $\Sigma_i\bm{b}_i\bm{b}_i^{T}$
\begin{itemize}
	\item given the inverse-Wishart prior

\begin{equation*}
 \begin{split} 
  p(\bm{\Psi})& \propto IW(\nu, \bm{S}_{0}) \\
  & \propto |\bm{\Psi}|^{-\frac{(\nu + k + 1)}{2}}exp\left(-\frac{1}{2}tr(\bm{S}_0\bm{\Psi}^{-1}) \right)
 \end{split}
\end{equation*}

the full conditional posterior of $\bm{\Psi}$ is

\begin{equation}
 \begin{split} 
  p(\bm{\Psi}|.)& \propto |\bm{\Psi}|^{-\frac{(\nu + n + k + 1)}{2}}exp\left(-\frac{1}{2}tr([\bm{S}_0+\bm{S}_b]\bm{\Psi}^{-1}) \right) \\
  & \propto IW(\nu + n, \bm{S}_{0} + \bm{S}_{b})
 \end{split}
\end{equation}	

where $\nu = 2$

	\item inverse-Wishart \textit{a là} Huang and Wand
	
\begin{equation*}
	\begin{split}
	p(\bm{\Psi}|a_1, a_2)& \propto IW(\nu + k - 1, 2\nu diag(1/a_1, 1/a_2)), \\
	a_k& \propto IG(\eta,1/A_k^2) \\
	p(\bm{\Psi})& \propto |\bm{\Psi}|^{-\frac{(\nu+k-1+1)}{2}}exp\left(-\frac{1}{2}tr(2\nu diag(1/a_1, 1/a_2)\bm{\Psi}^{-1}) \right) \\
	& \times \left(\frac{1}{a_1}\right)^{\eta+1}exp\left(-\frac{1}{A_1^2a_1}\right) \times \left(\frac{1}{a_2}\right)^{\eta+1}exp\left(-\frac{1}{A_2^2a_2}\right) \\
	\end{split}
\end{equation*}

the full conditional posterior of $\bm{\Psi}$ is

\begin{equation}
 \begin{split} 
  p(\bm{\Psi}|.)& \propto |\bm{\Psi}|^{-\frac{(\nu+k-1+n+1)}{2}}exp\left(-\frac{1}{2}tr([\bm{S}_b+2\nu diag(1/a_1, 1/a_2)]\bm{\Psi}^{-1}) \right) \\
  & \propto IW(\nu+k-1+n, \bm{S}_{b}+2\nu diag(1/a_1, 1/a_2))\\
  p(a_k|.)& \propto IG\left(\eta(\nu+k), \nu\left(\bm{\Psi}^{-1}_{kk}+\frac{1}{A_k^2}\right)\right)
 \end{split}
\end{equation}		

where $\eta = \frac{1}{2}, \nu = 2, k = 2$, and $n$ is the number of clusters (individuals). (For the conditional posterior of $a_k$ refer to Huang and Wand (2013), section 4.2).
	
	\item Matrix-F variate

Following section 2.3 in Mulder and Pericchi (2018), instead of working directly with the $\bm{\Psi} \sim F(\nu, \delta, \bm{B})$ we apply the parameter expansion defined above (see section on priors) and model it as $\bm{\Psi} \sim IW(\delta + k - 1, \bm{\Omega})$ with $\bm{\Omega} \sim W(\nu, \bm{B})$. With this parameter expansion, the conditional priors are: 
\begin{equation*}
 \begin{split} 
  \bm{\Psi|\Omega}& \sim IW(\delta+k-1, \bm{\Omega}) \\
  \bm{\Omega|\Psi}& \sim W(\nu+\delta+k-1, (\bm{\Psi}^{-1} + \bm{B}^{-1})^{-1}) \\
 \end{split}
\end{equation*}
% Old workout of matrix F (keep for a bit)
%\begin{equation*}
% \begin{split} 
%  p(\bm{\Psi})& \propto F(\bm{\Psi}; \nu, \delta, \bm{B}) \\
%  & \propto \int IW(\bm{\Psi}; \delta + k - 1, \bm{\Omega}) \times W(\bm{\Omega}; \nu, \bm{B})d\bm{\Omega} \\
%  & \propto \int |\bm{\Psi}|^{-\frac{(\delta+2k)}{2}}exp\left(-\frac{1}{2}tr(\bm{\Omega}\bm{\Psi}^{-1})\right) \times |\bm{\Omega}|^{-\frac{(\nu-k-1)}{2}}exp\left(-\frac{1}{2}tr(\bm{\Omega}\bm{B}^{-1})\right)d\bm{\Omega}
% \end{split}
%\end{equation*}

which makes the full conditional posterior of:
\begin{equation*}
 \begin{split} 
  \bm{\Psi|\Omega},.& \sim IW(\delta+k-1+n, \bm{S}_b + \bm{\Omega}) \\
  \bm{\Omega|\Psi},.& \sim W(\nu+\delta+k-1, (\bm{\Psi}^{-1} + \bm{B}^{-1})^{-1}) \\
 \end{split}
\end{equation*}
% Old workout of matrix F (keep for a bit)
%\begin{equation}
% \begin{split} 
%  p(\bm{\Psi}|.)& \propto \int |\bm{\Psi}|^{-\frac{(\delta+2k+n)}{2}}exp\left(-\frac{1}{2}tr([\bm{\Omega}+\bm{S}_b]\bm{\Psi}^{-1})\right) \times |\bm{\Omega}|^{-\frac{(\nu-k-1)}{2}}exp\left(-\frac{1}{2}tr(\bm{\Omega}\bm{B}^{-1})\right)d\bm{\Omega} \\
%  & \propto \int IW(\delta+k+n-1, \bm{\Omega} + \bm{S}_{b}) \times W(\nu, \bm{B})d\bm{\Omega}
% \end{split}
%\end{equation}			

with parameters as defined above. Given these posteriors, the Gibbs sampler implementation is straightforward.
	
\end{itemize}

\paragraph{Notation Conventions}

\begin{itemize}
	\item $n$ number of clusters; $i$ specific cluster
	\item $J$ number of observations within cluster; $j$ specific observation
	\item $N$ total number of observations
\end{itemize}



\end{document}


